{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_project_ssast.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_eMIXlmOpQwZ",
        "F7t3by8PKbQx",
        "_vJiEnMqNdd8",
        "E_VzcT75NWFs",
        "m6vKZ6yI9heI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Bbwjz4XerM6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the repo"
      ],
      "metadata": {
        "id": "_eMIXlmOpQwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/YuanGongND/ssast.git\n",
        "!git clone https://github.com/roeizig/ssast.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxGk2IUlpTaa",
        "outputId": "94d48d57-9f9d-415c-9eaf-555bd5d66d3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ssast'...\n",
            "remote: Enumerating objects: 3997, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 3997 (delta 100), reused 92 (delta 89), pack-reused 3879\u001b[K\n",
            "Receiving objects: 100% (3997/3997), 294.10 MiB | 26.56 MiB/s, done.\n",
            "Resolving deltas: 100% (333/333), done.\n",
            "Checking out files: 100% (3383/3383), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements"
      ],
      "metadata": {
        "id": "wZbV32LapLVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ssast/\n",
        "!pip install -r /content/ssast/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "im__hXpnpLEG",
        "outputId": "020d05cc-8540-498c-b949-093a0a3c248f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llvmlite==0.36.0\n",
            "  Downloading llvmlite-0.36.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 2.0 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.4.2\n",
            "  Downloading matplotlib-3.4.2-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 37.6 MB/s \n",
            "\u001b[?25hCollecting numba==0.53.1\n",
            "  Downloading numba-0.53.1-cp37-cp37m-manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.20.3\n",
            "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 42.9 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting scipy==1.7.0\n",
            "  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/ssast/requirements.txt (line 7)) (0.0)\n",
            "Collecting timm==0.4.5\n",
            "  Downloading timm-0.4.5-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 32.9 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.5 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.9.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 30.4 MB/s \n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting zipp==3.4.1\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r /content/ssast/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r /content/ssast/requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r /content/ssast/requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r /content/ssast/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r /content/ssast/requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.53.1->-r /content/ssast/requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->-r /content/ssast/requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->-r /content/ssast/requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.4.5->-r /content/ssast/requirements.txt (line 8)) (0.13.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->-r /content/ssast/requirements.txt (line 9)) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.2->-r /content/ssast/requirements.txt (line 2)) (1.15.0)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 1.3 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.5->-r /content/ssast/requirements.txt (line 8)) (2.23.0)\n",
            "  Downloading torchvision-0.11.3-cp37-cp37m-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.7 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 1.5 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 2.2 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 80.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=4847da49548ca16ba01eb4e2465eec68666946075c83c2f57ea613328ad54ed9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: numpy, torch, scipy, torchvision, scikit-learn, llvmlite, zipp, wget, torchaudio, timm, numba, matplotlib\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.0+cu113\n",
            "    Uninstalling torchvision-0.13.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.0+cu113\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.0\n",
            "    Uninstalling llvmlite-0.39.0:\n",
            "      Successfully uninstalled llvmlite-0.39.0\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.8.1\n",
            "    Uninstalling zipp-3.8.1:\n",
            "      Successfully uninstalled zipp-3.8.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.0+cu113\n",
            "    Uninstalling torchaudio-0.12.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.0+cu113\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.0\n",
            "    Uninstalling numba-0.56.0:\n",
            "      Successfully uninstalled numba-0.56.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.20.3 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.9.0 which is incompatible.\n",
            "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.7.0 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.20.3 which is incompatible.\u001b[0m\n",
            "Successfully installed llvmlite-0.36.0 matplotlib-3.4.2 numba-0.53.1 numpy-1.20.3 scikit-learn-0.24.2 scipy-1.7.0 timm-0.4.5 torch-1.9.0 torchaudio-0.9.0 torchvision-0.10.0 wget-3.2 zipp-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "F7t3by8PKbQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from ssast.src.models.ast_models import *"
      ],
      "metadata": {
        "id": "H5FzqwPkKdxR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overriding the model"
      ],
      "metadata": {
        "id": "jpiRZB7aJGsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New model definition"
      ],
      "metadata": {
        "id": "_vJiEnMqNdd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ssast.src.models.ast_models import ASTModel\n",
        "\n",
        "class ASTModelRoei(ASTModel):\n",
        "    def __init__(self):\n",
        "      super().__init__(fshape=16, tshape=16, fstride=10, tstride=10, \n",
        "                      input_fdim=128, input_tdim=1024, model_size='tiny', \n",
        "                      pretrain_stage=False, load_pretrained_mdl_path='/content/ssast/pretrained_model/SSAST-Tiny-Patch-400.pth')\n",
        "      # regressor head for fine-tuning / inference\n",
        "      # self.regressor_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim),\n",
        "      #                               nn.Linear(self.original_embedding_dim, 1))\n",
        "      self.regressor_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim),\n",
        "                                    nn.Linear(self.original_embedding_dim, 1))\n",
        "\n",
        "\n",
        "    def finetuning_regressor(self, x, y):\n",
        "        # Finetuning using the regressor task.\n",
        "        # Calculate MSE loss and update weights accordingly\n",
        "        # B = x.shape[0]\n",
        "\n",
        "        # Calculate RMSE loss and update weights\n",
        "        pred = self.regressor(x)\n",
        "        target = y\n",
        "        # If cannot use reference in a vectorized manner, use the for loop below\n",
        "        # pred = torch.empty((B), device=x.device).float()  # e.g. size 12 for a batch of 12 spectrograms\n",
        "        # target = torch.empty((B), device=x.device).float() # e.g. size 12 for a batch of 12 spectrograms\n",
        "        # for i in range(B):\n",
        "        #     #  +2 for indexes because of cls and dis token\n",
        "        #     pred[i] = self.regressor(x[i])\n",
        "        #     target[i] = y[i]\n",
        "        # calculate the RMSE loss\n",
        "        # rmse = torch.mean((pred - target) ** 2)**0.5\n",
        "\n",
        "        # Update weights according to MSE loss\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "        optimizer.zero_grad()\n",
        "        criterion = nn.MSELoss()\n",
        "        loss = criterion(pred, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def regressor(self, x):\n",
        "        # Copied from \"finetuningavgtok\"\n",
        "        # changed only the last part, sending the intermediate result\n",
        "        # to the regressor head instead of the mlp head.\n",
        "        # Inference only mode\n",
        "        B = x.shape[0]\n",
        "        x = self.v.patch_embed(x)\n",
        "        if self.cls_token_num == 2:\n",
        "            cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
        "            dist_token = self.v.dist_token.expand(B, -1, -1)\n",
        "            x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
        "        else:\n",
        "            cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
        "            x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.v.pos_embed\n",
        "        x = self.v.pos_drop(x)\n",
        "\n",
        "        for blk_id, blk in enumerate(self.v.blocks):\n",
        "            x = blk(x)\n",
        "        x = self.v.norm(x)\n",
        "\n",
        "        # average output of all tokens except cls token(s)\n",
        "        x = torch.mean(x[:, self.cls_token_num:, :], dim=1)\n",
        "        x = self.regressor_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x, task, y=None, cluster=True, mask_patch=400):\n",
        "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = x.transpose(2, 3)\n",
        "\n",
        "        # finetuning (ft), use the mean of all token (patch) output as clip-level representation.\n",
        "        # this is default for SSAST fine-tuning as during pretraining, supervision signal is given to each token, not the [cls] token\n",
        "        if task == 'ft_avgtok':\n",
        "            return self.finetuningavgtok(x)\n",
        "        # fine tuning the model as a regressor\n",
        "        elif task == 'ft_regressor':\n",
        "            return self.finetuning_regressor(x, y)\n",
        "        # inference using a regressor\n",
        "        elif task == 'regressor':\n",
        "            return self.regressor(x)\n",
        "        # alternatively, use the [cls] token output as clip-level representation.\n",
        "        elif task == 'ft_cls':\n",
        "            return self.finetuningcls(x)\n",
        "        # pretraining, masked patch classification (discriminative objective)\n",
        "        elif task == 'pretrain_mpc':\n",
        "            return self.mpc(x, mask_patch=mask_patch, cluster=cluster)\n",
        "        # pretraining, masked patch reconstruction (generative objective)\n",
        "        elif task == 'pretrain_mpg':\n",
        "            return self.mpg(x, mask_patch=mask_patch, cluster=cluster)\n",
        "        elif task == 'visualize_mask':\n",
        "            return self.mpc(x, mask_patch=mask_patch, cluster=cluster, show_mask=True)\n",
        "        else:\n",
        "            raise Exception('Task unrecognized.')\n",
        "\n"
      ],
      "metadata": {
        "id": "2NFeLyKqJL9W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the overridden model"
      ],
      "metadata": {
        "id": "E_VzcT75NWFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = ASTModelRoei()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEBsV1wvNbWV",
        "outputId": "878d4a89-8033-4c03-fc61-420d5aa58c1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now load a SSL pretrained models from /content/ssast/pretrained_model/SSAST-Tiny-Patch-400.pth\n",
            "pretraining patch split stride: frequency=16, time=16\n",
            "pretraining patch shape: frequency=16, time=16\n",
            "pretraining patch array dimension: frequency=8, time=64\n",
            "pretraining number of patches=512\n",
            "fine-tuning patch split stride: frequncey=10, time=10\n",
            "fine-tuning number of patches=1212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yLdU9hbkQjp",
        "outputId": "a0cabaaf-1862-494b-cfc6-5b131383ea89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ASTModelRoei(\n",
              "  (v): DistilledVisionTransformer(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(1, 192, kernel_size=(16, 16), stride=(10, 10))\n",
              "    )\n",
              "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "    (blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Mlp(\n",
              "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (act): GELU()\n",
              "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "    (pre_logits): Identity()\n",
              "    (head): Linear(in_features=192, out_features=1000, bias=True)\n",
              "    (head_dist): Linear(in_features=192, out_features=1000, bias=True)\n",
              "  )\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=192, out_features=527, bias=True)\n",
              "  )\n",
              "  (regressor_head): Sequential(\n",
              "    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=192, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the dataset"
      ],
      "metadata": {
        "id": "m6vKZ6yI9heI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Creating Dataset\n",
        "from random import sample\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class SpectrogramData(Dataset):\n",
        "    \n",
        "    def __init__(self, audio_dir, device):\n",
        "        self.audio_dir = audio_dir\n",
        "        self.device = device\n",
        "        self._create_ann()\n",
        "        self.transform = transforms.PILToTensor()\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.annotations[0])\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self._get_sample_path(index)\n",
        "        label = float(self._get_sample_label(index))\n",
        "\n",
        "        signal = Image.open(sample_path)\n",
        "        signal = self.transform(signal)\n",
        "        signal = signal.to(self.device)\n",
        "        return signal, label\n",
        "\n",
        "            \n",
        "    def _create_ann(self, path=\"ssast/spectrogram_images/\"):\n",
        "        Ann = [[],[],[]]\n",
        "        for subdir, dirs, files in os.walk(path):\n",
        "            for file in files:\n",
        "                if file.endswith('png'):\n",
        "                                Ann[0].append(file) ##name\n",
        "                                Ann[1].append(subdir) #folder\n",
        "                                tmp_label = file.split('_')[-1]\n",
        "                                Ann[2].append(float(tmp_label[:-4])) #label\n",
        "        Ann = np.asarray(Ann)\n",
        "        self.annotations = Ann\n",
        "\n",
        "\n",
        "    def shuffle(self):\n",
        "      perm = torch.randperm(len(self.annotations[0]))\n",
        "      self.annotations= self.annotations[:,perm]\n",
        "      pass    \n",
        "    \n",
        "\n",
        "    def _get_sample_path(self, index):\n",
        "        folder = self.annotations[1][index]\n",
        "        filename = self.annotations[0][index]\n",
        "        path = os.path.join(self.audio_dir, folder, filename)\n",
        "        return path\n",
        "    \n",
        "\n",
        "    def _get_sample_label(self, index):\n",
        "        label = self.annotations[2][index]\n",
        "        return label\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = 'cpu'\n",
        "    path =  \"ssast/spectrogram_images/\"\n",
        "    x = SpectrogramData(\"\", device)\n",
        " \n",
        "    trainval , test = torch.utils.data.random_split(x, [3000, 329],\n",
        "                                                generator=torch.Generator().manual_seed(42))\n",
        "    trainset, valset = torch.utils.data.random_split(trainval, [2990, 10],\n",
        "                                                    generator=torch.Generator().manual_seed(42))\n",
        " \n",
        "    xx = x[0]\n",
        "    spectgram = xx[0][0].cpu()\n",
        "\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDdJYvU18v5J",
        "outputId": "f78374cc-08eb-42fc-b1b4-c6962388267b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:165: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  img = torch.as_tensor(np.asarray(pic))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model on a single spectrogram"
      ],
      "metadata": {
        "id": "g4zBP_s-D8Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model(spectgram.unsqueeze(0).to(dtype=torch.float), task='regressor', y=torch.Tensor([float(xx[-1])]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n403DMxaGmhi",
        "outputId": "f6f4b845-f294-4a05-e0a5-a803381d9daa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3086]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model(spectgram.unsqueeze(0).to(dtype=torch.float), task='ft_regressor', y=torch.Tensor([[float(xx[-1])]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZAUwtUYBYcu",
        "outputId": "3da72294-ad95-4307-8f88-c6decba2cf5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3086]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model on a whole batch"
      ],
      "metadata": {
        "id": "weDWTYVZR41O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size,\n",
        "                            shuffle=True, num_workers=0)\n",
        "valloader = DataLoader(valset, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "for batch in trainloader:\n",
        "    break"
      ],
      "metadata": {
        "id": "yFYsUHWhStli"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model(batch[0].squeeze(1).to(dtype=torch.float), task='ft_regressor', y=batch[1].unsqueeze(-1).to(dtype=torch.float))"
      ],
      "metadata": {
        "id": "-hsg9FlTMo0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4638c742-1341-4ce7-a8a7-0c35aa5545f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.1600],\n",
              "        [8.1583],\n",
              "        [8.1533],\n",
              "        [8.1613],\n",
              "        [8.1532],\n",
              "        [8.1588],\n",
              "        [8.1536],\n",
              "        [8.1582],\n",
              "        [8.1599],\n",
              "        [8.1470],\n",
              "        [8.1544],\n",
              "        [8.1592],\n",
              "        [8.1583],\n",
              "        [8.1553],\n",
              "        [8.1587],\n",
              "        [8.1590]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}